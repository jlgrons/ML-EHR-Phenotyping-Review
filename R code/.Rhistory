cal_deep_median(bert, "AUROC")
cal_deep_median <- function(df, metrics = "AUROC") {
perf_summary <- ml_deep_metrics(df, metrics) %>% filter(DL_better != "")
deep <- perf_summary %>%
filter(Method == "Deep supervised learning") %>%
select(!!sym(metrics)) %>% pull()
trad <- perf_summary %>%
filter(Method == "Traditional supervised learning") %>%
select(!!sym(metrics)) %>% pull()
n_pheno <- length(unique(perf_summary$Study1))
n_study <- length(unique(str_extract(perf_summary$Study1, "[0-9]+$")))
# library(nonpar)
# print(mediantest(x = deep, y = trad, exact=TRUE))
print(paste(n_study, "articles studied", n_pheno, "phenotypes, and reported",
metrics, "with median", round(median(deep - trad),3)))
}
# BERT.
cal_deep_median(bert, "AUROC")
cal_deep_median(bert, "Sensitivity")
cal_deep_median(bert, "PPV")
### DL v.s. traditional ML that reported PPV and sensitivity
dl_summary <- read.csv("../data/dl_performance.csv")
cal_deep_median(dl_summary, "AUROC")
cal_deep_median(dl_summary, "Sensitivity")
cal_deep_median(dl_summary, "PPV")
# Case 1: improvement on both sensitivity and ppv.
ppv_summary <- ml_deep_metrics(dl_summary, "PPV") %>% filter(PPV != "")
deep_ppv <- ppv_summary %>% filter(Method == "Deep supervised learning")
trad_ppv <- ppv_summary %>% filter(Method == "Traditional supervised learning")
sens_summary <- ml_deep_metrics(dl_summary, "Sensitivity") %>% filter(Sensitivity != "")
deep_sens <- sens_summary %>% filter(Method == "Deep supervised learning")
trad_sens <- sens_summary %>% filter(Method == "Traditional supervised learning")
sens_improve <- deep_sens %>% filter(diff >= 0)
ppv_improve <- deep_ppv %>% filter(diff >= 0)
both_improvement <- sens_improve %>% inner_join(ppv_improve, by = "Study1")
# Median improve in sensitivity.
median(both_improvement$diff.x)
deep <- both_improvement$Sensitivity
trad <- trad_sens %>% filter(Study1 %in% both_improvement$Study1)
trad <- trad$Sensitivity
library(nonpar)
mediantest(x = deep, y = trad, exact=TRUE)
# Median improve in PPV.
median(both_improvement$diff.y)
deep <- both_improvement$PPV
trad <- trad_ppv %>% filter(Study1 %in% both_improvement$Study1)
trad <- trad$PPV
mediantest(x = deep, y = trad, exact=TRUE)
# Case 2: improvement on only sensitivity.
ppv_decrease <- deep_ppv %>% filter(diff < 0)
sens_improvement <- sens_improve %>% inner_join(ppv_decrease, by = "Study1")
# Median improve in sensitivity.
median(sens_improvement$diff.x)
deep <- sens_improvement$Sensitivity
trad <- trad_sens %>% filter(Study1 %in% sens_improvement$Study1)
trad <- trad$Sensitivity
mediantest(x = deep, y = trad, exact=TRUE)
both_improvement
# Case 3: improvement on only ppv.
sens_decrease <- deep_sens %>% filter(diff < 0)
ppv_improvement <- ppv_improve %>% inner_join(sens_decrease, by = "Study1")
# Median improve in sensitivity.
median(ppv_improvement$diff.x)
deep <- ppv_improvement$PPV
trad <- trad_ %>% filter(Study1 %in% ppv_improvement$Study1)
# Case 3: improvement on only ppv.
sens_decrease <- deep_sens %>% filter(diff < 0)
ppv_improvement <- ppv_improve %>% inner_join(sens_decrease, by = "Study1")
# Median improve in sensitivity.
median(ppv_improvement$diff.x)
deep <- ppv_improvement$PPV
trad <- trad_ %>% filter(Study1 %in% ppv_improvement$Study1)
# Case 3: improvement on only ppv.
sens_decrease <- deep_sens %>% filter(diff < 0)
ppv_improvement <- ppv_improve %>% inner_join(sens_decrease, by = "Study1")
# Median improve in sensitivity.
median(ppv_improvement$diff.x)
deep <- ppv_improvement$PPV
trad <- trad_ppv %>% filter(Study1 %in% ppv_improvement$Study1)
trad <- trad$PPV
mediantest(x = deep, y = trad, exact=TRUE)
# Case 2: improvement on only sensitivity.
ppv_decrease <- deep_ppv %>% filter(diff < 0)
sens_improvement <- sens_improve %>% inner_join(ppv_decrease, by = "Study1")
# Median improve in sensitivity.
median(sens_improvement$diff.x)
deep <- sens_improvement$Sensitivity
trad <- trad_sens %>% filter(Study1 %in% sens_improvement$Study1)
trad <- trad$Sensitivity
mediantest(x = deep, y = trad, exact=TRUE)
# Case 3: improvement on only ppv.
sens_decrease <- deep_sens %>% filter(diff < 0)
ppv_improvement <- ppv_improve %>% inner_join(sens_decrease, by = "Study1")
# Median improve in sensitivity.
median(ppv_improvement$diff.x)
deep <- ppv_improvement$PPV
trad <- trad_ppv %>% filter(Study1 %in% ppv_improvement$Study1)
trad <- trad$PPV
mediantest(x = deep, y = trad, exact=TRUE)
# Case 2: improvement on only sensitivity.
ppv_decrease <- deep_ppv %>% filter(diff < 0)
sens_improvement <- sens_improve %>% inner_join(ppv_decrease, by = "Study1")
# Median improve in sensitivity.
median(sens_improvement$diff.x)
deep <- sens_improvement$Sensitivity
trad <- trad_sens %>% filter(Study1 %in% sens_improvement$Study1)
trad <- trad$Sensitivity
mediantest(x = deep, y = trad, exact=TRUE)
# Stratified by different networks types.
dl_summary <- unnest_string_var(dl_summary, "DL_method")
rnn <- dl_summary %>% filter(DL_method_unnested == "RNN")
cnn <- dl_summary %>% filter(DL_method_unnested == "CNN")
bert <- dl_summary %>% filter(DL_method_unnested == "BERT")
# RNN.
cal_deep_median(rnn, "AUROC")
cal_deep_median(rnn, "Sensitivity")
cal_deep_median(rnn, "PPV")
cal_deep_median <- function(df, metrics = "AUROC") {
perf_summary <- ml_deep_metrics(df, metrics) %>% filter(DL_better != "")
deep <- perf_summary %>%
filter(Method == "Deep supervised learning") %>%
select(!!sym(metrics)) %>% pull()
trad <- perf_summary %>%
filter(Method == "Traditional supervised learning") %>%
select(!!sym(metrics)) %>% pull()
n_pheno <- length(unique(perf_summary$Study1))
n_study <- length(unique(str_extract(perf_summary$Study1, "[0-9]+$")))
library(nonpar)
print(mediantest(x = deep, y = trad, exact=TRUE))
print(paste(n_study, "articles studied", n_pheno, "phenotypes, and reported",
metrics, "with median", round(median(deep - trad),3)))
}
# RNN.
cal_deep_median(rnn, "AUROC")
cal_deep_median(rnn, "Sensitivity")
cal_deep_median(rnn, "PPV")
# CNN.
cal_deep_median(cnn, "AUROC")
cal_deep_median(cnn, "Sensitivity")
cal_deep_median(cnn, "PPV")
# BERT.
cal_deep_median(bert, "AUROC")
# Stratified by different networks types.
dl_summary <- unnest_string_var(dl_summary, "DL_method")
### DL v.s. traditional ML that reported PPV and sensitivity
dl_summary <- read.csv("../data/dl_performance.csv")
cal_deep_median(dl_summary, "AUROC")
cal_deep_median(dl_summary, "Sensitivity")
cal_deep_median(dl_summary, "PPV")
# Stratified by different networks types.
dl_summary <- unnest_string_var(dl_summary, "DL_method")
rnn <- dl_summary %>% filter(DL_method_unnested == "RNN")
cnn <- dl_summary %>% filter(DL_method_unnested == "CNN")
bert <- dl_summary %>% filter(DL_method_unnested == "BERT")
ffnn <- dl_summary %>% filter(DL_method_unnested == "FFNN")
ffnn
# FFNN
cal_deep_median(ffnn, "AUROC")
cal_deep_median(ffnn, "Sensitivity")
# FFNN
cal_deep_median(ffnn, "AUROC")
#cal_deep_median(ffnn, "Sensitivity")
cal_deep_median(ffnn, "PPV")
# FFNN
cal_deep_median(ffnn, "AUROC")
#cal_deep_median(ffnn, "Sensitivity")
#cal_deep_median(ffnn, "PPV")
cal_deep_median <- function(df, metrics = "AUROC") {
perf_summary <- ml_deep_metrics(df, metrics) %>% filter(DL_better != "")
deep <- perf_summary %>%
filter(Method == "Deep supervised learning") %>%
select(!!sym(metrics)) %>% pull()
trad <- perf_summary %>%
filter(Method == "Traditional supervised learning") %>%
select(!!sym(metrics)) %>% pull()
n_pheno <- length(unique(perf_summary$Study1))
n_study <- length(unique(str_extract(perf_summary$Study1, "[0-9]+$")))
# library(nonpar)
# print(mediantest(x = deep, y = trad, exact=TRUE))
print(paste(n_study, "articles studied", n_pheno, "phenotypes, and reported",
metrics, "with median", round(median(deep - trad),3)))
}
# FFNN
cal_deep_median(ffnn, "AUROC")
cal_deep_median(ffnn, "Sensitivity")
cal_deep_median(ffnn, "PPV")
# BERT
cal_deep_median(bert, "AUROC")
cal_deep_median(bert, "Sensitivity")
cal_deep_median(bert, "PPV")
# FFNN
cal_deep_median(ffnn, "AUROC")
cal_deep_median(ffnn, "Sensitivity")
cal_deep_median(ffnn, "PPV")
nrow(df %>% filter(Openly_available_data == 0))
data_unnested <- unnest_string_var(df, "Data_type")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
# Load necessary packages
library(ggplot2)
library(tidyverse)
library(kableExtra)
library(ggsci)
library(grid)
library(gridExtra)
library(ggvenn)
library(ggpattern)
library(cowplot)
# Functions for analysis
source('analysis_functions.R')
# Read in annotation file
df_all <- read.csv("../data/annotations050822.csv", skip = 1)
df <- df_all %>% filter(Include_FR_SY == 1)
# Create variable indicating use of unstructured data
df$Unstructured <- (df$Unstructured_data_type != "")
# Create variable indicating use of structured data
df$Structured <- (df$Structured_data_type != "")
# Create variable indicating types of data
df$Data_type[df$Structured == 1] <- "Structured"
df$Data_type[df$Unstructured == 1] <- "Free-text"
df$Data_type[df$Unstructured + df$Structured == 2] <- "Both structured and free-text"
# As the paper used non-language unstructured data and structured data,
# we include it as structured data.
df$Data_type[df$Unstructured_data_language == "Non-language"] <- "Structured"
# Create variable indicating use of traditional ML method
df$Traditional[df$DL == ''] <- "Traditional machine learning"
df$Traditional[df$DL != ''] <- "Deep learning"
plot_validate_metrics(traditional_supervised, comparator = "rule", large_fig = FALSE)
# Read in annotation file
df_all <- read.csv("../data/annotations050822.csv", skip = 1)
df <- df_all %>% filter(Include_FR_SY == 1)
# Create variable indicating use of unstructured data
df$Unstructured <- (df$Unstructured_data_type != "")
# Create variable indicating use of structured data
df$Structured <- (df$Structured_data_type != "")
# Create variable indicating types of data
df$Data_type[df$Structured == 1] <- "Structured"
df$Data_type[df$Unstructured == 1] <- "Free-text"
df$Data_type[df$Unstructured + df$Structured == 2] <- "Both structured and free-text"
# As the paper used non-language unstructured data and structured data,
# we include it as structured data.
df$Data_type[df$Unstructured_data_language == "Non-language"] <- "Structured"
# Create variable indicating use of traditional ML method
df$Traditional[df$DL == ''] <- "Traditional machine learning"
df$Traditional[df$DL != ''] <- "Deep learning"
plot_validate_metrics(deep_supervised, comparator = "deep", large_fig = FALSE)
# Figure to plot ML vs DL across all metrics.
plot_deep_compare <- function(df,
comparison = "deep",
study = "Phenotype_ref",
large_fig = FALSE,
legend = FALSE) {
theme_type <- theme(axis.title.y = element_blank())
p2 <- plot_metrics(df, comparison, study, "Specificity", large_fig = large_fig) + theme_type
if (legend == TRUE) {
p2 <- p2 + theme(legend.position = "bottom")
} else {
p2 <- p2 + theme(legend.position = "none")
}
p1 <- plot_metrics(df, comparison, study, "Sensitivity", large_fig = large_fig)
p4 <- plot_metrics(df, comparison, study, "PPV", large_fig = large_fig) + theme_type
p5 <- plot_metrics(df, comparison, study, "AUROC", large_fig = large_fig) + theme_type
egg::ggarrange(p1, p2, p4, p5, nrow = 1, draw = FALSE)
}
plot_metrics <- function(df,
comparison = "rule",
study = "Phenotype",
metric = "Sensitivity",
large_fig = FALSE,
xlim = 0.01) {
# Remove all the metrics are NAs.
#df1 <- apply(df[, c(which(colnames(df) ==
#"Best_performing_Sensitivity")):ncol(df)], 2, as.numeric)
if (comparison == "rule") {
df <- ml_rule_metrics(df, metric)
} else if (comparison == "weakly") {
df <- weakly_metrics(df, metric)
} else if (comparison == "deep") {
# Check missingness and fill the NA.
if (typeof(df) == "double") {
df1 <- t(data.frame(df))
missing_index <- rowMeans(is.na(df1))
if (sum(missing_index == 1) > 0) {
df <- df[-which(missing_index == 1), ]
}
}
df <- ml_deep_metrics(df, metric)
} else {
df <- weakly_rule_metrics(df, metric)
}
title_size <- if_else(large_fig, 18, 8)
text_size <- if_else(large_fig, 10, 5)
legend_size <- if_else(large_fig, 15, 7)
point_size <- if_else(large_fig, 3, 2)
vjust_size <- if_else(large_fig, 0.5, 0.25)
df %>%
ggplot(aes(x = !!sym(study),
y = as.numeric(!!sym(metric)),
color = Method)) +
scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
scale_y_continuous(limits = c(xlim, 1),
labels = function(y) label_parsed(paste0(y*100))) +
scale_color_jama() +
coord_flip() +
labs(x = "", y = "", title = metric) +
theme_bw() +
geom_point(size = point_size) +
theme(legend.position = "none",
plot.title = element_text(size = title_size, hjust = 0.5, face="bold"),
axis.title.x = element_blank(),
axis.ticks.y = element_blank(),
axis.text.x = element_text(size = text_size, color = "black"),
axis.text.y = element_text(size = legend_size, vjust = vjust_size),
legend.title = element_blank(),
legend.text = element_text(size = legend_size))
}
plot_validate_metrics(deep_supervised, comparator = "deep", large_fig = FALSE)
# Figure to plot ML vs DL across all metrics.
plot_deep_compare <- function(df,
comparison = "deep",
study = "Phenotype_ref",
large_fig = FALSE,
legend = FALSE) {
theme_type <- theme(axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank())
p2 <- plot_metrics(df, comparison, study, "Specificity", large_fig = large_fig) + theme_type
if (legend == TRUE) {
p2 <- p2 + theme(legend.position = "bottom")
} else {
p2 <- p2 + theme(legend.position = "none")
}
p1 <- plot_metrics(df, comparison, study, "Sensitivity", large_fig = large_fig)
p4 <- plot_metrics(df, comparison, study, "PPV", large_fig = large_fig) + theme_type
p5 <- plot_metrics(df, comparison, study, "AUROC", large_fig = large_fig) + theme_type
egg::ggarrange(p1, p2, p4, p5, nrow = 1, draw = FALSE)
}
plot_metrics <- function(df,
comparison = "rule",
study = "Phenotype",
metric = "Sensitivity",
large_fig = FALSE,
xlim = 0.01) {
# Remove all the metrics are NAs.
#df1 <- apply(df[, c(which(colnames(df) ==
#"Best_performing_Sensitivity")):ncol(df)], 2, as.numeric)
if (comparison == "rule") {
df <- ml_rule_metrics(df, metric)
} else if (comparison == "weakly") {
df <- weakly_metrics(df, metric)
} else if (comparison == "deep") {
# Check missingness and fill the NA.
if (typeof(df) == "double") {
df1 <- t(data.frame(df))
missing_index <- rowMeans(is.na(df1))
if (sum(missing_index == 1) > 0) {
df <- df[-which(missing_index == 1), ]
}
}
df <- ml_deep_metrics(df, metric)
} else {
df <- weakly_rule_metrics(df, metric)
}
title_size <- if_else(large_fig, 18, 8)
text_size <- if_else(large_fig, 10, 5)
legend_size <- if_else(large_fig, 15, 7)
point_size <- if_else(large_fig, 3, 2)
vjust_size <- if_else(large_fig, 0.5, 0.25)
df %>%
ggplot(aes(x = !!sym(study),
y = as.numeric(!!sym(metric)),
color = Method)) +
scale_x_discrete(labels = function(x) str_wrap(x, width = 30)) +
scale_y_continuous(limits = c(xlim, 1),
labels = function(y) label_parsed(paste0(y*100))) +
scale_color_jama() +
coord_flip() +
labs(x = "", y = "", title = metric) +
theme_bw() +
geom_point(size = point_size) +
theme(legend.position = "none",
plot.title = element_text(size = title_size, hjust = 0.5, face="bold"),
axis.title.x = element_blank(),
axis.ticks.y = element_blank(),
axis.text.x = element_text(size = text_size, color = "black"),
axis.text.y = element_text(size = legend_size, vjust = vjust_size),
legend.title = element_blank(),
legend.text = element_text(size = legend_size))
}
plot_validate_metrics(deep_supervised, comparator = "deep", large_fig = FALSE)
# Figure to plot ML vs DL across all metrics.
plot_deep_compare <- function(df,
comparison = "deep",
study = "Phenotype_ref",
large_fig = FALSE,
legend = FALSE) {
# Remove y-axis label and text for graph other than sensitivity.
theme_type <- theme(axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank())
legend_pos = if_else(legend, "bottom", "none")
p1 <- plot_metrics(df, comparison, study, "Sensitivity", large_fig = large_fig)
p4 <- plot_metrics(df, comparison, study, "PPV", large_fig = large_fig) + theme_type
p5 <- plot_metrics(df, comparison, study, "AUROC", large_fig = large_fig) + theme_type
p2 <- plot_metrics(df, comparison, study, "Specificity", large_fig = large_fig) +
theme(legend.position = legend_pos) +
theme_type
egg::ggarrange(p1, p2, p4, p5, nrow = 1, draw = FALSE)
}
plot_validate_metrics(deep_supervised, comparator = "deep", large_fig = FALSE)
# Figure to plot ML vs rule across all metrics.
plot_rule_compare <- function(df,
comparison = "rule",
study = "Phenotype_ref",
large_fig = FALSE,
legend = FALSE) {
# Remove y-axis label and text for graph other than sensitivity.
theme_type <- theme(axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank())
legend_pos = if_else(legend, "bottom", "none")
p1 <- plot_metrics(df, comparison, study, metric = "Sensitivity", large_fig)
p2 <- plot_metrics(df, comparison, study, metric = "Specificity", large_fig) + theme_type
p3 <- plot_metrics(df, comparison, study, metric = "PPV", large_fig) +
theme(legend.position = legend_pos) +
theme_type
p4 <- plot_metrics(df, comparison, study, metric = "NPV", large_fig) + theme_type
p5 <- plot_metrics(df, comparison, study, metric = "AUROC", large_fig) + theme_type
egg::ggarrange(p1,p2,p3,p4,p5, nrow = 1, draw = FALSE)
}
# Figure to plot ML vs DL across all metrics.
plot_deep_compare <- function(df,
comparison = "deep",
study = "Phenotype_ref",
large_fig = FALSE,
legend = FALSE) {
# Remove y-axis label and text for graph other than sensitivity.
theme_type <- theme(axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank())
legend_pos = if_else(legend, "bottom", "none")
p1 <- plot_metrics(df, comparison, study, "Sensitivity", large_fig = large_fig)
p4 <- plot_metrics(df, comparison, study, "PPV", large_fig = large_fig) + theme_type
p5 <- plot_metrics(df, comparison, study, "AUROC", large_fig = large_fig) + theme_type
p2 <- plot_metrics(df, comparison, study, "Specificity", large_fig = large_fig) +
theme(legend.position = legend_pos) +
theme_type
egg::ggarrange(p1, p2, p4, p5, nrow = 1, draw = FALSE)
}
plot_validate_metrics(traditional_supervised, comparator = "rule", large_fig = FALSE)
plot_validate_metrics(deep_supervised, comparator = "deep", large_fig = FALSE)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
# Load necessary packages
library(ggplot2)
library(tidyverse)
library(kableExtra)
library(ggsci)
library(grid)
library(gridExtra)
library(ggvenn)
library(ggpattern)
library(cowplot)
# Functions for analysis
source('analysis_functions.R')
# Read in annotation file
df_all <- read.csv("../data/annotations050822.csv", skip = 1)
df <- df_all %>% filter(Include_FR_SY == 1)
# Create variable indicating use of unstructured data
df$Unstructured <- (df$Unstructured_data_type != "")
# Create variable indicating use of structured data
df$Structured <- (df$Structured_data_type != "")
# Create variable indicating types of data
df$Data_type[df$Structured == 1] <- "Structured"
df$Data_type[df$Unstructured == 1] <- "Free-text"
df$Data_type[df$Unstructured + df$Structured == 2] <- "Both structured and free-text"
# As the paper used non-language unstructured data and structured data,
# we include it as structured data.
df$Data_type[df$Unstructured_data_language == "Non-language"] <- "Structured"
# Create variable indicating use of traditional ML method
df$Traditional[df$DL == ''] <- "Traditional machine learning"
df$Traditional[df$DL != ''] <- "Deep learning"
df$ML_type <- factor(df$ML_type, levels = c("Supervised",
"Semi-supervised",
"Weakly-supervised",
"Unsupervised"))
stacked_bar(df, "ML_type", "Traditional", order_by_count = FALSE)
# Traditional ML method.
method_unnested <- unnest_string_var(df, "Traditional_ML_method")
print_tables(method_unnested, "ML_type", "Traditional_ML_method_unnested", top_count = 20, restric_count = TRUE,
title = "Common traditional machine learning methods (Count > 1)")
print_multiple_items(method_unnested, "traditional machine learning methods")
# DL method.
method_unnested <- unnest_string_var(df, "DL_method")
print_tables(method_unnested,
"DL_method_unnested",
"ML_type",
top_count = 10,
title = "Deep supervised learning methods",
restric_count = TRUE)
print_multiple_items(method_unnested, "deep learning methods")
traditional_supervised <- df %>% filter(ML_type == "Supervised") %>% filter(DL_method == '')
deep_supervised <- df %>% filter(ML_type == "Supervised") %>% filter(DL_method != '')
semi_supervised <- df %>% filter(ML_type == "Semi-supervised")
weakly_supervised <- df %>% filter(ML_type == "Weakly-supervised")
un_supervised <- df %>% filter(ML_type == "Unsupervised")
traditional_supervised$Category <- "Traditional supervised"
deep_supervised$Category <- "Deep supervised"
semi_supervised$Category <- "Semi-supervised"
weakly_supervised$Category <- "Weakly-supervised"
un_supervised$Category <- "Unsupervised"
plot_validate_metrics(traditional_supervised, comparator = "rule", large_fig = FALSE)
plot_validate_metrics(traditional_supervised, comparator = "rule", large_fig = TRUE)
plot_validate_metrics(traditional_supervised, comparator = "rule", large_fig = TRUE)
